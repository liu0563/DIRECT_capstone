{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import json\n",
    "import gensim\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers import Input, Bidirectional, LSTM, regularizers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, MaxPooling2D, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '../../wyns/data/tweet_global_warming.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet existence  \\\n",
       "0  Global warming report urges governments to act...       Yes   \n",
       "1  Fighting poverty and global warming in Africa ...       Yes   \n",
       "2  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "3  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "4  URUGUAY: Tools Needed for Those Most Vulnerabl...       Yes   \n",
       "\n",
       "   existence.confidence  \n",
       "0                1.0000  \n",
       "1                1.0000  \n",
       "2                0.8786  \n",
       "3                1.0000  \n",
       "4                0.8087  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(filename, encoding='latin')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "word_vector_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(txt, vocab=None, replace_char=' ',\n",
    "                max_length=300, pad_out=False,\n",
    "                to_lower=True, reverse = False,\n",
    "                truncate_left=False, encoding=None,\n",
    "                letters_only=False):\n",
    "  \n",
    "    txt = txt.split()\n",
    "    # Remove HTML\n",
    "    # This will keep characters and other symbols\n",
    "    txt = [re.sub(r'http:.*', '', r) for r in txt]\n",
    "    txt = [re.sub(r'https:.*', '', r) for r in txt]\n",
    "    \n",
    "    txt = ( \" \".join(txt))\n",
    "    # Remove non-emoticon punctuation and numbers\n",
    "    txt = re.sub(\"[.,!0-9]\", \" \", txt)\n",
    "    if letters_only: \n",
    "        txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n",
    "    txt = \" \".join(txt.split())\n",
    "    # store length for multiple comparisons\n",
    "    txt_len = len(txt)\n",
    "\n",
    "    if truncate_left:\n",
    "        txt = txt[-max_length:]\n",
    "    else:\n",
    "        txt = txt[:max_length]\n",
    "    # change case\n",
    "    if to_lower:\n",
    "        txt = txt.lower()\n",
    "    # Reverse order\n",
    "    if reverse:\n",
    "        txt = txt[::-1]\n",
    "    # replace chars\n",
    "    if vocab is not None:\n",
    "        txt = ''.join([c if c in vocab else replace_char for c in txt])\n",
    "    # re-encode text\n",
    "    if encoding is not None:\n",
    "        txt = txt.encode(encoding, errors=\"ignore\")\n",
    "    # pad out if needed\n",
    "    if pad_out and max_length>txt_len:\n",
    "        txt = txt + replace_char * (max_length - txt_len)\n",
    "    if txt.find('@') > -1:\n",
    "        for i in range(len(txt.split('@'))-1):\n",
    "            try:\n",
    "                if str(txt.split('@')[1]).find(' ') > -1:\n",
    "                    to_remove = '@' + str(txt.split('@')[1].split(' ')[0]) + \" \"\n",
    "                else:\n",
    "                    to_remove = '@' + str(txt.split('@')[1])\n",
    "                txt = txt.replace(to_remove,'')\n",
    "            except:\n",
    "                pass\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance(df):\n",
    "    print(\"Balancing the classes\")\n",
    "    type_counts = df['Sentiment'].value_counts()\n",
    "    min_count = min(type_counts.values)\n",
    "\n",
    "    balanced_df = None\n",
    "    for key in type_counts.keys():\n",
    "\n",
    "        df_sub = df[df['Sentiment']==key].sample(n=min_count, replace=False)\n",
    "        if balanced_df is not None:\n",
    "            balanced_df = balanced_df.append(df_sub)\n",
    "        else:\n",
    "            balanced_df = df_sub\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_to_sentiment(tweet):\n",
    "    norm_text = normalize(tweet[0])\n",
    "    if tweet[1] in ('Yes', 'Y'):\n",
    "        return ['positive', norm_text]\n",
    "    elif tweet[1] in ('No', 'N'):\n",
    "        return ['negative', norm_text]\n",
    "    else:\n",
    "        return ['other', norm_text]\n",
    "    \n",
    "df = pd.read_csv(filename, encoding='latin')\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    data.append(tweet_to_sentiment(row))\n",
    "        \n",
    "twitter = pd.DataFrame(data, columns=['Sentiment', 'clean_text'], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>global warming report urges governments to act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>fighting poverty and global warming in africa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>carbon offsets: how a vatican forest failed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>carbon offsets: how a vatican forest failed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>uruguay: tools needed for those most vulnerabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                         clean_text\n",
       "0  positive  global warming report urges governments to act...\n",
       "1  positive  fighting poverty and global warming in africa ...\n",
       "2  positive  carbon offsets: how a vatican forest failed to...\n",
       "3  positive  carbon offsets: how a vatican forest failed to...\n",
       "4  positive  uruguay: tools needed for those most vulnerabl..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this demo lets just keep one and five stars the others are marked 'other\n",
    "twitter = twitter[twitter['Sentiment'].isin(['positive', 'negative'])]\n",
    "print(len(twitter))\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment                                                                                                               positive\n",
      "clean_text    global warming report urges governments to act|brussels belgium (ap) - the world faces increased hunger and [link]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "print(twitter.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to balance training data\n",
    "# twitter = balance(twitter)\n",
    "# len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now go from the pandas into lists of text and labels\n",
    "text = twitter['clean_text'].values\n",
    "labels_0 = pd.get_dummies(twitter['Sentiment'])  # mapping of the labels with dummies (has headers)\n",
    "labels = labels_0.values # removes the headers\n",
    "# Perform the Train/test split\n",
    "X_train_, X_test_, Y_train_, Y_test_ = train_test_split(text,labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Now for a simple bidirectional LSTM algorithm we set our feature sizes and train a tokenizer\n",
    "# First we Tokenize and get the data into a form that the model can read - this is BoW\n",
    "# In this cell we are also going to define some of our hyperparameters\n",
    "max_fatures = 2000\n",
    "max_len=300\n",
    "batch_size = 32\n",
    "embed_dim = 300\n",
    "lstm_out = 140\n",
    "\n",
    "dense_out=len(labels[0]) #length of features\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(X_train_)\n",
    "X_train = tokenizer.texts_to_sequences(X_train_)\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
    "X_test = tokenizer.texts_to_sequences(X_test_)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(max_fatures, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_len:\n",
    "        continue\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "    if word in word_vector_model.vocab:\n",
    "        embedding_matrix[i] = word_vector_model.word_vec(word)\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = True to fine tune the embeddings\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            embed_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_fatures,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2000, 300)         600000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 280)               493920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 562       \n",
      "=================================================================\n",
      "Total params: 1,094,482\n",
      "Trainable params: 494,482\n",
      "Non-trainable params: 600,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model using the pre-trained embedding\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Bidirectional(LSTM(lstm_out, recurrent_dropout=0.5, activation='tanh'))(embedded_sequences)\n",
    "preds = Dense(dense_out, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3380 samples, validate on 845 samples\n",
      "Epoch 1/20\n",
      " - 130s - loss: 0.5342 - acc: 0.7485 - val_loss: 0.4716 - val_acc: 0.7893\n",
      "Epoch 2/20\n",
      " - 130s - loss: 0.4675 - acc: 0.7914 - val_loss: 0.4584 - val_acc: 0.7905\n",
      "Epoch 3/20\n"
     ]
    }
   ],
   "source": [
    "model_hist_embedding = model.fit(X_train, Y_train_, epochs = 20, batch_size=batch_size, verbose = 2,\n",
    "                        validation_data=(X_test,Y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test_[:,1], np.round(model.predict(X_test))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "x = np.arange(20)+1\n",
    "fig=plt.figure(dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, model_hist_embedding.history['acc'])\n",
    "ax.plot(x, model_hist_embedding.history['val_acc'])\n",
    "ax.legend(['Training', 'Testing'], loc='lower right')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.45,1.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"LSTM Accuracy\")\n",
    "plt.show()\n",
    "fig.savefig(fname='03.png', bbox_inches='tight', format='png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_hist_embedding.model.save(\"../../wyns/data/climate_sentiment_m2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../../wyns/data/climate_sentiment_m3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_hist_embedding.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_sentiment(tweet):\n",
    "    # Review is coming in as Y/N/NaN\n",
    "    # this then cleans the summary and review and gives it a positive or negative value\n",
    "    norm_text = normalize(tweet[0])\n",
    "    if tweet[1] in ('Yes', 'Y'):\n",
    "        return ['positive', norm_text]\n",
    "    elif tweet[1] in ('No', 'N'):\n",
    "        return ['negative', norm_text]\n",
    "    else:\n",
    "        return ['other', norm_text]\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    norm_text = normalize(tweet[0])\n",
    "    return [tweet[1], tweet[2], norm_text, tweet[3], tweet[4], tweet[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweets.txt\", delimiter=\"~~n~~\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    data.append(clean_tweet(row))\n",
    "twitter = pd.DataFrame(data, columns=['long', 'lat', 'clean_text', 'time', 'retweets', 'location'], dtype=str)\n",
    "to_predict_ = twitter['clean_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now for a simple bidirectional LSTM algorithm we set our feature sizes and train a tokenizer\n",
    "# First we Tokenize and get the data into a form that the model can read - this is BoW\n",
    "# In this cell we are also going to define some of our hyperparameters\n",
    "\n",
    "max_fatures = 2000\n",
    "max_len=300\n",
    "batch_size = 32\n",
    "embed_dim = 300\n",
    "lstm_out = 140\n",
    "\n",
    "dense_out=len(labels[0]) #length of features\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(to_predict_)\n",
    "to_predict = tokenizer.texts_to_sequences(to_predict_)\n",
    "to_predict = pad_sequences(to_predict, maxlen=max_len, padding='post')\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"negative predictions: {}\".format(sum(np.round(predictions)[:,0])))\n",
    "print(\"positive predictions: {}\".format(sum(np.round(predictions)[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame([twitter['long'], twitter['lat'], twitter['clean_text'],\n",
    "                      twitter['time'], twitter['retweets'], twitter['location'], predictions[:,0], predictions[:,1]]).T\n",
    "df_out = df_out.rename(index=str, columns={\"Unnamed 0\": \"negative\", \"Unnamed 1\": \"positive\"})\n",
    "print(df_out.shape)\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(\"sample_prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
