{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on using python-twitter for direct capstone\n",
    "\n",
    "first pip install python-twitter: `pip install python-twitter`\n",
    "\n",
    "if you want, [readthedocs](https://python-twitter.readthedocs.io/en/latest/installation.html)\n",
    "\n",
    "then setup a twitter app. I followed these [instructions](https://iag.me/socialmedia/how-to-create-a-twitter-app-in-8-easy-steps/)\n",
    "\n",
    "The site seems to have changed a bit since that post but, more or less the same.\n",
    "\n",
    "The important thing is to obtain your consumer key/secret and access key/secret.\n",
    "\n",
    "You would instantiate them with twitter with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import pandas as pd\n",
    "import json\n",
    "import gensim\n",
    "import collections\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a way to handle our private twitter tokens\n",
    "#   A skeletal .tweetrc file:\n",
    "#     [Tweet]\n",
    "#     consumer_key: *consumer_key*\n",
    "#     consumer_secret: *consumer_password*\n",
    "#     access_key: *access_key*\n",
    "#     access_secret: *access_password*\n",
    "class TweetRc(object):\n",
    "    def __init__(self):\n",
    "        self._config = None\n",
    "\n",
    "    def GetConsumerKey(self):\n",
    "        return self._GetOption('consumer_key')\n",
    "\n",
    "    def GetConsumerSecret(self):\n",
    "        return self._GetOption('consumer_secret')\n",
    "\n",
    "    def GetAccessKey(self):\n",
    "        return self._GetOption('access_key')\n",
    "\n",
    "    def GetAccessSecret(self):\n",
    "        return self._GetOption('access_secret')\n",
    "\n",
    "    def _GetOption(self, option):\n",
    "        try:\n",
    "            return self._GetConfig().get('Tweet', option)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _GetConfig(self):\n",
    "        if not self._config:\n",
    "            self._config = configparser.ConfigParser()\n",
    "            self._config.read(os.path.expanduser('~/.tweetrc')) \n",
    "        return self._config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw = TweetRc()\n",
    "api=twitter.Api(consumer_key=tw.GetConsumerKey(),\n",
    "consumer_secret=tw.GetConsumerSecret(),\n",
    "access_token_key=tw.GetAccessKey(),\n",
    "access_token_secret=tw.GetAccessSecret())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potentially **very** *neat* we can load up samples of live tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TwitterError",
     "evalue": "{'message': 'Exceeded connection limit for user'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/twitter/api.py\u001b[0m in \u001b[0;36m_ParseAndCheckTwitter\u001b[0;34m(self, json_data)\u001b[0m\n\u001b[1;32m   4874\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4875\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4876\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTwitterError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-d652e79459d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         for line in api.GetStreamFilter(track=['global warming', 'climate',\n\u001b[1;32m      6\u001b[0m                                                'sustainability', 'pollution'],\n\u001b[0;32m----> 7\u001b[0;31m                                        languages=['en'], filter_level=['low']):\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/twitter/api.py\u001b[0m in \u001b[0;36mGetStreamFilter\u001b[0;34m(self, follow, track, locations, languages, delimited, stall_warnings, filter_level)\u001b[0m\n\u001b[1;32m   4571\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4572\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4573\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ParseAndCheckTwitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4574\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/twitter/api.py\u001b[0m in \u001b[0;36m_ParseAndCheckTwitter\u001b[0;34m(self, json_data)\u001b[0m\n\u001b[1;32m   4880\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTwitterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Technical Error\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4881\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"Exceeded connection limit for user\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4882\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTwitterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Exceeded connection limit for user\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4883\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"Error 401 Unauthorized\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4884\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTwitterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Unauthorized\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTwitterError\u001b[0m: {'message': 'Exceeded connection limit for user'}"
     ]
    }
   ],
   "source": [
    "# I'm curious what other filters we should use. Can we screen for tweeets that only\n",
    "# also contain long/lat data?\n",
    "timeout = time.time() + 10 #10 seconds from now\n",
    "while True:\n",
    "    with open(\"tweet_feed.json\", 'w+') as f:\n",
    "        for line in api.GetStreamFilter(track=['global warming', 'climate',\n",
    "                                               'sustainability', 'pollution'],\n",
    "                                       languages=['en'], filter_level=['low']):\n",
    "            f.write(json.dumps(line))\n",
    "            f.write(\"\\r\")\n",
    "            timeDiff = datetime.datetime.now() - startTime\n",
    "            if time.time() > timeout:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['contributors', 'coordinates', 'created_at', 'display_text_range',\n",
       "       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',\n",
       "       'favorited', 'filter_level', 'geo', 'id', 'id_str',\n",
       "       'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
       "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
       "       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place',\n",
       "       'possibly_sensitive', 'quote_count', 'quoted_status',\n",
       "       'quoted_status_id', 'quoted_status_id_str', 'reply_count',\n",
       "       'retweet_count', 'retweeted', 'retweeted_status', 'source', 'text',\n",
       "       'timestamp_ms', 'truncated', 'user'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see what data is available for a given tweet\n",
    "df = pd.read_json(\"big_tweet.json\",  lines=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @StopAdaniCairns: Seems #ClimateChange doesn’t concern #Farmers\n",
      "\n",
      "Ignore #drought #floods #bushfire #cyclones \n",
      "\n",
      "#Agriculture production t…\n"
     ]
    }
   ],
   "source": [
    "#looks like some classic climate change fodder\n",
    "print(df[\"text\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Sarah's most recent model (75% accuracy on test set)\n",
    "from keras.models import load_model\n",
    "model = load_model(\"../../core/data/climate_sentiment_m1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "####These should be ported into core module\n",
    "def read_data(data_file):\n",
    "    for i, line in enumerate (data_file): \n",
    "        # do some pre-processing and return a list of words for each review text\n",
    "        yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "def build_dataset(vocab, n_words):\n",
    "    \"\"\"Process the top n_words from raw inputs (vocab from read_data) into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] #stores when word is found --> UNK = unknown \n",
    "    count.extend(collections.Counter(vocab).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    token = list() \n",
    "    unk_count = 0\n",
    "    for word in vocab: #\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK'] assigned to 0 \n",
    "            unk_count += 1\n",
    "        token.append(index) #outputs a list of integers that represent words\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) #allows for word lookup by integer\n",
    "    return token, count, dictionary, reversed_dictionary\n",
    "\n",
    "###I'm being lazy and using 3 million words. need to shorten to whats been seen in our training data\n",
    "def create_word_vec():\n",
    "    #Get word vectors using googles pretrained word2vec  \n",
    "    #takes a minute \n",
    "    google = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)\n",
    "\n",
    "    #includes some stop words (i.e. the, also, should, but not a, and, of)\n",
    "    #includes misspellings \n",
    "    #includes commony paired words (i.e. New_York)\n",
    "\n",
    "    vocab = google.vocab.keys()\n",
    "    total_vocab = len(vocab)\n",
    "    print (\"Set includes\", total_vocab, \"words\")\n",
    "\n",
    "    # Copy word vectors and delete Word2Vec model  and original corpus to save memory\n",
    "    X_vecs = google.wv\n",
    "    #del google #wait to explore model first \n",
    "    del google \n",
    "    return X_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 1996\n",
      "using gensim to preprocess: ['rt', 'stopadanicairns', 'seems', 'climatechange', 'doesn', 'concern', 'farmers', 'ignore', 'drought', 'floods', 'bushfire', 'cyclones', 'agriculture', 'production']\n"
     ]
    }
   ],
   "source": [
    "top_words = 20000 #use number higher than expected unique words\n",
    "\n",
    "tweet_vocab = list(read_data(df['text']))\n",
    "flat_tweet_vocab = [item for sublist in tweet_vocab for item in sublist]\n",
    "token, count, dictionary, reversed_dictionary = build_dataset(flat_tweet_vocab, top_words)\n",
    "\n",
    "print(\"Number of unique words: {}\".format(len(count))) #correct num of unique words \n",
    "#using gensim simple preprocesser \n",
    "print ('using gensim to preprocess:', tweet_vocab[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set includes 3000000 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesleybeckner/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "###This is much slow\n",
    "X_vecs = create_word_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "# Generate random indexes\n",
    "X = df['text']\n",
    "test_split = 0\n",
    "train_size = int(len(X)*test_split)\n",
    "test_size = len(X) - train_size\n",
    "vector_size = 300\n",
    "window_size = 10\n",
    "max_tweet_length=512\n",
    "\n",
    "indexes = set(np.random.choice(len(tweet_vocab), train_size + test_size, replace=False))\n",
    "X_train = np.zeros((train_size, max_tweet_length, vector_size))\n",
    "Y_train = np.zeros((train_size, 3), dtype=np.int32)\n",
    "X_test = np.zeros((test_size, max_tweet_length, vector_size))\n",
    "Y_test = np.zeros((test_size, 3), dtype=np.int32)\n",
    "for i, index in enumerate(indexes):\n",
    "    for t, token in enumerate(tweet_vocab[index]):\n",
    "        if t >= max_tweet_length:\n",
    "            break\n",
    "        \n",
    "        if token not in X_vecs:\n",
    "            continue\n",
    "    \n",
    "        if i < train_size:\n",
    "            X_train[i, t, :] = X_vecs[token]\n",
    "        else:\n",
    "            X_test[i - train_size, t, :] = X_vecs[token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a really strong \"Yes\" classification from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet: RT @StopAdaniCairns: Seems #ClimateChange doesn’t concern #Farmers\n",
      "\n",
      "Ignore #drought #floods #bushfire #cyclones \n",
      "\n",
      "#Agriculture production t… \n",
      "\n",
      "sentiment analysis: [0.01564297 0.98653907 0.005147  ]\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print(\"tweet: {} \\n\\nsentiment analysis: {}\".format(df[\"text\"][i],Y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COOL!**\n",
    "\n",
    "My next questions:\n",
    "\n",
    "1. can we filter for only geo-tagged tweets? how many hits will we get a day then?\n",
    "2. what other word filters should we use (up to 400)?\n",
    "3. where do we want to store our data/handle calls to the api\n",
    " 1. api calls every half hour? \n",
    " 2. CNN predictions once a day? \n",
    " 3. through away all but sentiment class and location after running CNN?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
