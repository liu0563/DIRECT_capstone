{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Googles Word2Vec \n",
    "\n",
    "Shows how to use googles pretrained model as inputs to a CNN \n",
    "\n",
    "### First lets load in the pretrained model and do some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed (for reproducibility)\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set includes 3000000 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Get word vectors using googles pretrained word2vec  \n",
    "#takes a minute \n",
    "google = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)\n",
    "\n",
    "#includes some stop words (i.e. the, also, should, but not a, and, of)\n",
    "#includes misspellings \n",
    "#includes commony paired words (i.e. New_York)\n",
    "\n",
    "vocab = google.vocab.keys()\n",
    "total_vocab = len(vocab)\n",
    "print (\"Set includes\", total_vocab, \"words\")\n",
    "\n",
    "# Copy word vectors and delete Word2Vec model  and original corpus to save memory\n",
    "X_vecs = google.wv\n",
    "#del google #wait to explore model first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('global_warming', 0.889603853225708),\n",
       " ('Climate_Change', 0.7147639393806458),\n",
       " ('Climate', 0.6953692436218262),\n",
       " ('Global_warming', 0.661054253578186),\n",
       " ('climate', 0.6569506525993347),\n",
       " ('greenhouse_gas_emissions', 0.6449477076530457),\n",
       " ('greenhouse_gases', 0.6432511806488037),\n",
       " ('carbon_emissions', 0.6395047307014465),\n",
       " ('Global_Warming', 0.6281516551971436),\n",
       " ('reducing_carbon_emissions', 0.6227284669876099)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "google.wv.most_similar('climate_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del google #save mem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the vectors \n",
    "X_vecs['hello'].size #check vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now see how using pretrained vectors improves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 6090\n",
      "dataset without NaN: 6087\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "# One hot encode sentiment \n",
    "\n",
    "data = pd.read_csv(\"../wyns/data/tweet_global_warming.csv\", encoding=\"latin\")\n",
    "print(\"Full dataset: {}\".format(data.shape[0]))\n",
    "data['existence'].fillna(value='ambiguous', inplace = True) #replace NA's in existence with \"ambiguous\"\n",
    "data['existence'].replace(('Y', 'N'), ('Yes', 'No'), inplace=True) #rename so encoder doesnt get confused\n",
    "data = data.dropna() #now drop NA values\n",
    "print(\"dataset without NaN: {}\".format(data.shape[0]))\n",
    "X = data.iloc[:,0]\n",
    "Y = data.iloc[:,1]\n",
    "\n",
    "#one hot encoding = dummy vars from categorical var \n",
    "#Create a one-hot encoded binary matrix \n",
    "#N, Y, Ambig\n",
    "#1, 0, 0 \n",
    "#0, 1, 0\n",
    "#0, 0, 1\n",
    "\n",
    "#encode class as integers \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y) \n",
    "\n",
    "#convert integers to one hot encoded\n",
    "Y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_file):\n",
    "    for i, line in enumerate (data_file): \n",
    "        yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "def build_dataset(vocab, n_words):\n",
    "    \"\"\"Process the top n_words from raw inputs (vocab from read_data) into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] #stores when word is found --> UNK = unknown \n",
    "    count.extend(collections.Counter(vocab).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    token = list() \n",
    "    unk_count = 0\n",
    "    for word in vocab: #\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK'] assigned to 0 \n",
    "            unk_count += 1\n",
    "        token.append(index) #outputs a list of integers that represent words\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) #allows for word lookup by integer\n",
    "    return token, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 12117\n"
     ]
    }
   ],
   "source": [
    "top_words = 20000 #use number higher than expected unique words\n",
    "\n",
    "tweet_vocab = list(read_data(data['tweet']))\n",
    "flat_tweet_vocab = [item for sublist in tweet_vocab for item in sublist]\n",
    "token, count, dictionary, reversed_dictionary = build_dataset(flat_tweet_vocab, top_words)\n",
    "\n",
    "print(\"Number of unique words: {}\".format(len(count))) #correct num of unique words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "# Generate random indexes\n",
    "\n",
    "test_split = 0.8\n",
    "train_size = int(len(X)*test_split)\n",
    "test_size = len(X) - train_size\n",
    "vector_size = 300\n",
    "window_size = 10\n",
    "max_tweet_length=512\n",
    "\n",
    "indexes = set(np.random.choice(len(tweet_vocab), train_size + test_size, replace=False))\n",
    "\n",
    "X_train = np.zeros((train_size, max_tweet_length, vector_size))\n",
    "Y_train = np.zeros((train_size, 3), dtype=np.int32)\n",
    "X_test = np.zeros((test_size, max_tweet_length, vector_size))\n",
    "Y_test = np.zeros((test_size, 3), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(indexes):\n",
    "    for t, token in enumerate(tweet_vocab[index]):\n",
    "        if t >= max_tweet_length:\n",
    "            break\n",
    "        \n",
    "        if token not in X_vecs:\n",
    "            continue\n",
    "    \n",
    "        if i < train_size:\n",
    "            X_train[i, t, :] = X_vecs[token]\n",
    "        else:\n",
    "            X_test[i - train_size, t, :] = X_vecs[token]\n",
    "            \n",
    "            \n",
    "    if i < train_size:\n",
    "        Y_train[i, :] = Y[index]\n",
    "    else:\n",
    "        Y_test[i - train_size, :] = Y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at how our model performs now! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 512, 32)           28832     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 512, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 256, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               2048250   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 753       \n",
      "=================================================================\n",
      "Total params: 2,080,939\n",
      "Trainable params: 2,080,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4869 samples, validate on 1218 samples\n",
      "Epoch 1/2\n",
      "4869/4869 [==============================] - 41s 8ms/step - loss: 0.5755 - acc: 0.6904 - val_loss: 0.5578 - val_acc: 0.7124\n",
      "Epoch 2/2\n",
      "4869/4869 [==============================] - 41s 8ms/step - loss: 0.4663 - acc: 0.7755 - val_loss: 0.5273 - val_acc: 0.7332\n",
      "Accuracy: 73.32%\n"
     ]
    }
   ],
   "source": [
    "#Some variables \n",
    "\n",
    "top_words = 1000\n",
    "max_words = 150\n",
    "filters = 32 #filter = 1 x KERNEL \n",
    "\n",
    "# create the model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution1D(32, kernel_size=3, activation='elu', padding='same',\n",
    "                 input_shape=(max_tweet_length, vector_size)))\n",
    "\n",
    "model.add(Convolution1D(filters=filters, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=2, batch_size=128,\n",
    "    verbose=1)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
