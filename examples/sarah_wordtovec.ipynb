{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Word2Vec in Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-29d00a30c1a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 6090\n",
      "dataset without NaN: 6087\n",
      "Number of unique words: 5541\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../core/data/tweet_global_warming.csv\", encoding=\"latin\")\n",
    "print(\"Full dataset: {}\".format(data.shape[0]))\n",
    "data['existence'].fillna(value='ambiguous', inplace = True) #replace NA's in existence with \"ambiguous\"\n",
    "data['existence'].replace(('Y', 'N'), ('Yes', 'No'), inplace=True) #rename so encoder doesnt get confused\n",
    "data = data.dropna() #now drop NA values\n",
    "print(\"dataset without NaN: {}\".format(data.shape[0]))\n",
    "X = data.iloc[:,0]\n",
    "Y = data.iloc[:,1]\n",
    "print(\"Number of unique words: {}\".format(len(np.unique(np.hstack(X)))))\n",
    "\n",
    "#one hot encoding = dummy vars from categorical var \n",
    "#Create a one-hot encoded binary matrix \n",
    "#N, Y, Ambig\n",
    "#1, 0, 0 \n",
    "#0, 1, 0\n",
    "#0, 0, 1\n",
    "\n",
    "#encode class as integers \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y) \n",
    "\n",
    "#convert integers to one hot encoded\n",
    "Y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet existence  \\\n",
       "0  Global warming report urges governments to act...       Yes   \n",
       "1  Fighting poverty and global warming in Africa ...       Yes   \n",
       "2  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "3  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "4  URUGUAY: Tools Needed for Those Most Vulnerabl...       Yes   \n",
       "\n",
       "   existence.confidence  \n",
       "0                1.0000  \n",
       "1                1.0000  \n",
       "2                0.8786  \n",
       "3                1.0000  \n",
       "4                0.8087  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() #check head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous data point: \n",
      "tweet                   Virginia to Investigate Global Warming Scienti...\n",
      "existence                                                       ambiguous\n",
      "existence.confidence                                                    1\n",
      "Name: 6086, dtype: object\n",
      "One hot coding for ambiguous: [ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "#one hot example\n",
    "print(\"Ambiguous data point: \")\n",
    "print (data.iloc[6083])\n",
    "print (\"One hot coding for ambiguous: {}\".format(Y[6083]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average tweet length: \n",
      "111 characters\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZjnnRG9mlnNO9GZmOedEb2aWc/8fIfmbat/XYcMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2903df28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"average tweet length: \")\n",
    "result = list(map(len, X))\n",
    "print(\"{:.0f}\".format(np.mean(list(map(len, X)))), 'characters')\n",
    "plt.subplot(121)\n",
    "plt.boxplot(result)\n",
    "plt.subplot(122)\n",
    "plt.hist(result, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "X:  (6087,) Y:  (6087, 3)\n",
      "Classes:  [ 0.  1.]\n",
      "Number of unique words:  5541\n",
      "Tweet length: \n",
      "Mean 111 words (std: 27.51)\n"
     ]
    }
   ],
   "source": [
    "print (\"Training data: \")\n",
    "print (\"X: \", X.shape,\"Y: \", Y.shape)\n",
    "print (\"Classes: \", np.unique(Y))\n",
    "print(\"Number of unique words: \", len(np.unique(np.hstack(X)))) #based on encoding this is the number of unique words? Categorized by frequency \n",
    "\n",
    "# Summarize review length\n",
    "print(\"Tweet length: \")\n",
    "print(\"Mean %.0f words (std: %.2f)\" % (np.mean(result), np.std(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Word2Vec Stuff\n",
    "### First some dataset exploration\n",
    "* Most of this is just leftover from when I accidently started coding up Word2Vec lolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_file):\n",
    "    for i, line in enumerate (data_file): \n",
    "        # do some pre-processing and return a list of words for each review text\n",
    "        yield gensim.utils.simple_preprocess (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global', 'warming', 'report', 'urges', 'governments', 'to', 'act', 'brussels', 'belgium', 'ap']\n"
     ]
    }
   ],
   "source": [
    "data_file=\"../core/data/tweet_global_warming.csv\"\n",
    "vocab = list(read_data(data['tweet']))\n",
    "flat_vocab = [item for sublist in vocab for item in sublist] #need to flatten vocab list - couldnt figure out how to do with split \n",
    "#print (vocab[0:10]) #list of lists - bad \n",
    "print (flat_vocab[0:10]) #check that we have a single list of words - wahoo! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(vocab, n_words):\n",
    "    \"\"\"Process the top n_words from raw inputs (vocab from read_data) into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] #stores when word is found --> UNK = unknown \n",
    "    count.extend(collections.Counter(vocab).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    token = list() \n",
    "    unk_count = 0\n",
    "    for word in vocab: #\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK'] assigned to 0 \n",
    "            unk_count += 1\n",
    "        token.append(index) #outputs a list of integers that represent words\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) #allows for word lookup by integer\n",
    "    return token, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = 20000 #use number higher than expected unique words \n",
    "token, count, dictionary, reversed_dictionary = build_dataset(flat_vocab, top_words) #check to see if runs on flat_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12117"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary) #check num unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNK', 0],\n",
       " ('http', 3943),\n",
       " ('climate', 3757),\n",
       " ('change', 3438),\n",
       " ('global', 3244),\n",
       " ('warming', 3134),\n",
       " ('the', 2473),\n",
       " ('ly', 2401),\n",
       " ('bit', 2204),\n",
       " ('to', 1924)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count[0:10] #number of times first 11 words are used, \n",
    "\n",
    "#UNK = unknown words to dict, should be zero when top_words > max words in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Global warming report urges governments to act...\n",
       "1    Fighting poverty and global warming in Africa ...\n",
       "2    Carbon offsets: How a Vatican forest failed to...\n",
       "3    Carbon offsets: How a Vatican forest failed to...\n",
       "4    URUGUAY: Tools Needed for Those Most Vulnerabl...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets train a Word2Vec Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = list(read_data(data['tweet'])) #create a list of words in tweets \n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inputs \n",
    "#text = list of words \n",
    "#size = size of word vector **scaled from example that used 150 for a 18,000 word dataset**\n",
    "#window = max distance between target word and nearest word \n",
    "#min_count = min times a word must appear in data set (weeds out infrequent words)\n",
    "#sg=0, CBOW Training (best output)\n",
    "\n",
    "model = gensim.models.Word2Vec (text, size=50, window=10, min_count=2, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######OTHER MODELS TO CONSIDER########\n",
    "# we can also use gensims model.phrases module \n",
    "#bigram_transformer = gensim.models.Phrases(text)\n",
    "#model = Word2Vec(bigram_transformer[text], size=50, window=10, min_count=2)\n",
    "\n",
    "#or we could try using FastText another gensim model \n",
    "#FastText can be used on words not in dataset (unlike Word2Vec)\n",
    "#from gensim.models import FastText\n",
    "#model = FastText(text, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:3244, index:3, sample_int:872712862)\n",
      "[ 1.04918456  0.15850732 -1.40685773 -0.84766501 -0.87204534  0.57584053\n",
      " -0.1454823  -0.43417612 -0.41027182 -1.5447973  -0.32138428  0.97522813\n",
      " -0.67647725 -1.83529782 -0.51812011 -0.63785952  0.69422477  2.40389371\n",
      "  0.727458    0.69882929  1.50972807 -1.54841352 -0.6773392  -0.83626193\n",
      "  1.1212157   0.97462237  0.0263989  -2.44981337 -1.74876153  1.59723997\n",
      "  1.52616787 -0.07044011 -0.12189162  0.4257499   0.6161328   0.16896611\n",
      "  0.94026756  0.36409599  0.41795141  0.36264929  0.45958728 -2.36621046\n",
      " -0.07958051  1.52549458 -0.13357019  0.32000992  0.70338446 -0.73206228\n",
      "  0.80231112 -0.39504412]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv #store word vectors \n",
    "#word_vectors.vocab #dict of words \n",
    "print (word_vectors.vocab['global']) #example of stored in dict\n",
    "print (model.wv['global']) #example word vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('warming', 0.9966424107551575),\n",
       " ('then', 0.9962437152862549),\n",
       " ('or', 0.9961068630218506),\n",
       " ('all', 0.9957795143127441),\n",
       " ('you', 0.9957417249679565),\n",
       " ('if', 0.9957247376441956),\n",
       " ('heavily', 0.9946765899658203),\n",
       " ('believe', 0.994510293006897),\n",
       " ('snow', 0.9944669604301453),\n",
       " ('it', 0.994449257850647)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can do some cool NLP tasks\n",
    "\n",
    "model.wv.most_similar('global') #words most similar to global...lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dangerous'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"global warming is dangerous\".split()) #lol fun 2 play with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99664246635732945"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('global', 'warming') #solid for not using \"phrases\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now feed Word2Vec Embeddings into Model? \n",
    "--> Cant figure out yet, ignore this section haha <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Ocean Saltiness Getting Weird ¥Ë_ Blame Global Warming: IndyPosted (blog) A study conducted by scientists at Austral... http://bit.ly/cCQa1y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-fdd028c0391b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Ocean Saltiness Getting Weird ¥Ë_ Blame Global Warming: IndyPosted (blog) A study conducted by scientists at Austral... http://bit.ly/cCQa1y'"
     ]
    }
   ],
   "source": [
    "top_words = 1000\n",
    "max_words = 150\n",
    "word_vec = 32 #dont understand why we want a 32 bit vector\n",
    "test_split = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=test_split)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+1, word_vec, input_length=max_words)) #is it better to preconvert using word to vec? \n",
    "model.add(Convolution1D(filters=word_vec, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128,\n",
    "    verbose=1)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
