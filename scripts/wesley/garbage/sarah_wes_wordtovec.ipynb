{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Word2Vec in Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleybeckner/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 6090\n",
      "dataset without NaN: 6087\n",
      "Number of unique words: 5541\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../core/data/tweet_global_warming.csv\", encoding=\"latin\")\n",
    "print(\"Full dataset: {}\".format(data.shape[0]))\n",
    "data['existence'].fillna(value='ambiguous', inplace = True) #replace NA's in existence with \"ambiguous\"\n",
    "data['existence'].replace(('Y', 'N'), ('Yes', 'No'), inplace=True) #rename so encoder doesnt get confused\n",
    "data = data.dropna() #now drop NA values\n",
    "print(\"dataset without NaN: {}\".format(data.shape[0]))\n",
    "X = data.iloc[:,0]\n",
    "Y = data.iloc[:,1]\n",
    "print(\"Number of unique words: {}\".format(len(np.unique(np.hstack(X)))))\n",
    "\n",
    "#one hot encoding = dummy vars from categorical var \n",
    "#Create a one-hot encoded binary matrix \n",
    "#N, Y, Ambig\n",
    "#1, 0, 0 \n",
    "#0, 1, 0\n",
    "#0, 0, 1\n",
    "\n",
    "#encode class as integers \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y) \n",
    "\n",
    "#convert integers to one hot encoded\n",
    "Y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet existence  \\\n",
       "0  Global warming report urges governments to act...       Yes   \n",
       "1  Fighting poverty and global warming in Africa ...       Yes   \n",
       "2  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "3  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "4  URUGUAY: Tools Needed for Those Most Vulnerabl...       Yes   \n",
       "\n",
       "   existence.confidence  \n",
       "0                1.0000  \n",
       "1                1.0000  \n",
       "2                0.8786  \n",
       "3                1.0000  \n",
       "4                0.8087  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() #check head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous data point: \n",
      "tweet                   Virginia to Investigate Global Warming Scienti...\n",
      "existence                                                       ambiguous\n",
      "existence.confidence                                                    1\n",
      "Name: 6086, dtype: object\n",
      "One hot coding for ambiguous: [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#one hot example\n",
    "print(\"Ambiguous data point: \")\n",
    "print (data.iloc[6083])\n",
    "print (\"One hot coding for ambiguous: {}\".format(Y[6083]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average tweet length: \n",
      "111 characters\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTgQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12048b390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"average tweet length: \")\n",
    "result = list(map(len, X))\n",
    "print(\"{:.0f}\".format(np.mean(list(map(len, X)))), 'characters')\n",
    "plt.subplot(121)\n",
    "plt.boxplot(result)\n",
    "plt.subplot(122)\n",
    "plt.hist(result, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "X:  (6087,) Y:  (6087, 3)\n",
      "Classes:  [0. 1.]\n",
      "Number of unique words:  5541\n",
      "Tweet length: \n",
      "Mean 111 words (std: 27.51)\n"
     ]
    }
   ],
   "source": [
    "print (\"Training data: \")\n",
    "print (\"X: \", X.shape,\"Y: \", Y.shape)\n",
    "print (\"Classes: \", np.unique(Y))\n",
    "print(\"Number of unique words: \", len(np.unique(np.hstack(X)))) #based on encoding this is the number of unique words? Categorized by frequency \n",
    "\n",
    "# Summarize review length\n",
    "print(\"Tweet length: \")\n",
    "print(\"Mean %.0f words (std: %.2f)\" % (np.mean(result), np.std(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Word2Vec Stuff\n",
    "### First some dataset exploration\n",
    "* Most of this is just leftover from when I accidently started coding up Word2Vec lolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_file):\n",
    "    for i, line in enumerate (data_file): \n",
    "        # do some pre-processing and return a list of words for each review text\n",
    "        yield gensim.utils.simple_preprocess (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global', 'warming', 'report', 'urges', 'governments', 'to', 'act', 'brussels', 'belgium', 'ap']\n"
     ]
    }
   ],
   "source": [
    "data_file=\"../core/data/tweet_global_warming.csv\"\n",
    "vocab = list(read_data(data['tweet']))\n",
    "flat_vocab = [item for sublist in vocab for item in sublist] #need to flatten vocab list - couldnt figure out how to do with split \n",
    "#print (vocab[0:10]) #list of lists - bad \n",
    "print (flat_vocab[0:10]) #check that we have a single list of words - wahoo! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(vocab, n_words):\n",
    "    \"\"\"Process the top n_words from raw inputs (vocab from read_data) into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] #stores when word is found --> UNK = unknown \n",
    "    count.extend(collections.Counter(vocab).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    token = list() \n",
    "    unk_count = 0\n",
    "    for word in vocab: #\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK'] assigned to 0 \n",
    "            unk_count += 1\n",
    "        token.append(index) #outputs a list of integers that represent words\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) #allows for word lookup by integer\n",
    "    return token, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = 20000 #use number higher than expected unique words \n",
    "token, count, dictionary, reversed_dictionary = build_dataset(flat_vocab, top_words) #check to see if runs on flat_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12117"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary) #check num unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNK', 0],\n",
       " ('http', 3943),\n",
       " ('climate', 3757),\n",
       " ('change', 3438),\n",
       " ('global', 3244),\n",
       " ('warming', 3134),\n",
       " ('the', 2473),\n",
       " ('ly', 2401),\n",
       " ('bit', 2204),\n",
       " ('to', 1924)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count[0:10] #number of times first 11 words are used, \n",
    "\n",
    "#UNK = unknown words to dict, should be zero when top_words > max words in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Global warming report urges governments to act...\n",
       "1    Fighting poverty and global warming in Africa ...\n",
       "2    Carbon offsets: How a Vatican forest failed to...\n",
       "3    Carbon offsets: How a Vatican forest failed to...\n",
       "4    URUGUAY: Tools Needed for Those Most Vulnerabl...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets train a Word2Vec Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(read_data(data['tweet'])) #create a list of words in tweets \n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inputs \n",
    "#text = list of words \n",
    "#size = size of word vector **scaled from example that used 150 for a 18,000 word dataset**\n",
    "#window = max distance between target word and nearest word \n",
    "#min_count = min times a word must appear in data set (weeds out infrequent words)\n",
    "#sg=0, CBOW Training (best output)\n",
    "\n",
    "model = gensim.models.Word2Vec (text, size=50, window=10, min_count=2, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######OTHER MODELS TO CONSIDER########\n",
    "# we can also use gensims model.phrases module \n",
    "#bigram_transformer = gensim.models.Phrases(text)\n",
    "#model = Word2Vec(bigram_transformer[text], size=50, window=10, min_count=2)\n",
    "\n",
    "#or we could try using FastText another gensim model \n",
    "#FastText can be used on words not in dataset (unlike Word2Vec)\n",
    "#from gensim.models import FastText\n",
    "#model = FastText(text, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:3244, index:3, sample_int:872712862)\n",
      "[-0.91956913  1.102274   -1.3224545   1.1866059  -0.03569545 -1.4637282\n",
      " -0.3155923  -0.9504031  -1.176143   -0.63930297  0.8565027  -0.90950304\n",
      "  0.02040344  1.7277472   0.31625772 -0.20540822  1.2060884  -0.93768215\n",
      " -1.9144189   0.74558014  0.6843383  -1.4148959  -0.9346652   0.03052262\n",
      " -0.0972968   0.39970213 -0.45387214 -0.48926544 -0.03122804  0.17222334\n",
      " -0.3753483   0.96275157  0.49657938  1.910913    2.0302825   1.9741842\n",
      "  0.7019072  -0.13257375 -1.0610958   1.2323228   1.1585338   0.0604427\n",
      "  0.2534835  -0.32983482  1.3860939  -0.42764676  0.39380133  0.10162891\n",
      "  1.909356    2.1041582 ]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv #store word vectors \n",
    "#word_vectors.vocab #dict of words \n",
    "print (word_vectors.vocab['global']) #example of stored in dict\n",
    "print (model.wv['global']) #example word vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all', 0.9968538284301758),\n",
       " ('or', 0.9968318939208984),\n",
       " ('solve', 0.996817946434021),\n",
       " ('you', 0.9965774416923523),\n",
       " ('no', 0.996370792388916),\n",
       " ('imminent', 0.9962351322174072),\n",
       " ('if', 0.9961458444595337),\n",
       " ('can', 0.996083676815033),\n",
       " ('do', 0.9960814714431763),\n",
       " ('we', 0.9960352182388306)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can do some cool NLP tasks\n",
    "\n",
    "model.wv.most_similar('global') #words most similar to global...lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dangerous'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"global warming is dangerous\".split()) #lol fun 2 play with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995714780232951"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('global', 'warming') #solid for not using \"phrases\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the wv word vectors into a numpy matrix that is suitable for insertion\n",
    "# into our TensorFlow and Keras models\n",
    "vector_dim = 50 #from sarah\n",
    "embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now feed Word2Vec Embeddings into Model? \n",
    "--> Cant figure out yet, ignore this section haha <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed (for reproducibility)\n",
    "np.random.seed(1000)\n",
    "    \n",
    "# Gensim Word2Vec model\n",
    "vector_size = 512\n",
    "window_size = 10\n",
    "\n",
    "# Create Word2Vec\n",
    "word2vec = Word2Vec(sentences=X,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000)\n",
    "\n",
    "# Copy word vectors and delete Word2Vec model  and original corpus to save memory\n",
    "X_vecs = word2vec.wv\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_split = 0.8\n",
    "train_size = int(len(X)*test_split)\n",
    "test_size = len(X) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_tweet_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "corpus = X\n",
    "# Tokenize and stem\n",
    "tkr = RegexpTokenizer('[a-zA-Z0-9@]+')\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "tokenized_corpus = []\n",
    "\n",
    "for i, tweet in enumerate(corpus):\n",
    "    tokens = [stemmer.stem(t) for t in tkr.tokenize(tweet) if not t.startswith('@')]\n",
    "    tokenized_corpus.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "# Generate random indexes\n",
    "indexes = set(np.random.choice(len(tokenized_corpus), train_size + test_size, replace=False))\n",
    "\n",
    "X_train = np.zeros((train_size, max_tweet_length, vector_size))\n",
    "Y_train = np.zeros((train_size, 3), dtype=np.int32)\n",
    "X_test = np.zeros((test_size, max_tweet_length, vector_size))\n",
    "Y_test = np.zeros((test_size, 3), dtype=np.int32)\n",
    "\n",
    "for i, index in enumerate(indexes):\n",
    "    for t, token in enumerate(tokenized_corpus[index]):\n",
    "        if t >= max_tweet_length:\n",
    "            break\n",
    "        \n",
    "        if token not in X_vecs:\n",
    "            continue\n",
    "    \n",
    "        if i < train_size:\n",
    "            X_train[i, t, :] = X_vecs[token]\n",
    "        else:\n",
    "            X_test[i - train_size, t, :] = X_vecs[token]\n",
    "            \n",
    "            \n",
    "    if i < train_size:\n",
    "        Y_train[i, :] = Y[index]\n",
    "    else:\n",
    "        Y_test[i - train_size, :] = Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 512, 32)           49184     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 512, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 256, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               2048250   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 753       \n",
      "=================================================================\n",
      "Total params: 2,101,291\n",
      "Trainable params: 2,101,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4869 samples, validate on 1218 samples\n",
      "Epoch 1/4\n",
      "4869/4869 [==============================] - 71s 15ms/step - loss: 0.6055 - acc: 0.6798 - val_loss: 0.6225 - val_acc: 0.6689\n",
      "Epoch 2/4\n",
      "4869/4869 [==============================] - 64s 13ms/step - loss: 0.5905 - acc: 0.6824 - val_loss: 0.6042 - val_acc: 0.6730\n",
      "Epoch 3/4\n",
      "4869/4869 [==============================] - 62s 13ms/step - loss: 0.5779 - acc: 0.6971 - val_loss: 0.6041 - val_acc: 0.6762\n",
      "Epoch 4/4\n",
      "4869/4869 [==============================] - 60s 12ms/step - loss: 0.5712 - acc: 0.7003 - val_loss: 0.6156 - val_acc: 0.6683\n",
      "Accuracy: 66.83%\n"
     ]
    }
   ],
   "source": [
    "# top_words = 1000\n",
    "# max_words = 150\n",
    "word_vec = 32 #dont understand why we want a 32 bit vector\n",
    "# test_split = 0.3\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=test_split)\n",
    "\n",
    "# X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution1D(32, kernel_size=3, activation='elu', padding='same',\n",
    "                 input_shape=(max_tweet_length, vector_size)))\n",
    "\n",
    "model.add(Convolution1D(filters=word_vec, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=4, batch_size=128,\n",
    "    verbose=1)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.16%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
